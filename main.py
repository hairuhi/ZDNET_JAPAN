import os
import re
import json
from datetime import datetime, timedelta, timezone
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from googletrans import Translator


# ======== ì„¤ì • ë¡œë“œ ========
load_dotenv()

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")
TARGET_URL = os.getenv("TARGET_URL", "https://japan.zdnet.com/software/")
STORAGE_FILE = os.getenv("STORAGE_FILE", "seen_articles.json")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; ZDNetCrawler/1.0; +https://github.com/yourname)"
}

# JST (ì¼ë³¸ ì‹œê°„)
JST = timezone(timedelta(hours=9))

translator = Translator()


class ConfigError(Exception):
    pass


def ensure_config():
    missing = []
    if not TELEGRAM_BOT_TOKEN:
        missing.append("TELEGRAM_BOT_TOKEN")
    if not TELEGRAM_CHAT_ID:
        missing.append("TELEGRAM_CHAT_ID")

    if missing:
        raise ConfigError(
            "í™˜ê²½ë³€ìˆ˜ê°€ ë¶€ì¡±í•´ìš”: " + ", ".join(missing)
        )


def fetch_html(url: str) -> str:
    resp = requests.get(url, headers=HEADERS, timeout=15)
    resp.raise_for_status()
    return resp.text


def parse_title_and_datetime(raw_text: str):
    """
    ì•µì»¤ í…ìŠ¤íŠ¸ì—ì„œ
    '... 2025-11-16 10:01' í˜•íƒœì˜ ë‚ ì§œ/ì‹œê°„ì„ ë–¼ì–´ë‚´ê³ 
    (ì œëª©, datetime) ì„ ë¦¬í„´.
    datetime íŒŒì‹± ì‹¤íŒ¨ ì‹œ published_atì€ None.
    """
    if not raw_text:
        return "", None

    m = re.search(r"(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})\s*$", raw_text)
    if not m:
        # ë‚ ì§œê°€ ì—†ìœ¼ë©´ ì œëª©ë§Œ ë°˜í™˜
        return raw_text.strip(), None

    dt_str = m.group(1)
    title_part = raw_text[:m.start(1)].strip()
    try:
        published_at = datetime.strptime(dt_str, "%Y-%m-%d %H:%M").replace(tzinfo=JST)
    except ValueError:
        published_at = None

    return title_part, published_at


def extract_new_articles(html: str, base_url: str, now_jst: datetime):
    """
    'æ–°ç€' ì„¹ì…˜ì—ì„œ ì§€ë‚œ 24ì‹œê°„ ì´ë‚´ ê¸°ì‚¬ë§Œ ì¶”ì¶œ.
    ê° ì•„ì´í…œ: {title_ja_raw, title_ja, url, published_at}
    """
    soup = BeautifulSoup(html, "html.parser")

    # "æ–°ç€" í—¤ë” ì°¾ê¸° (h2 / h3)
    header = soup.find(
        lambda tag: tag.name in ["h2", "h3"]
        and tag.get_text(strip=True).startswith("æ–°ç€")
    )
    if not header:
        print("[WARN] 'æ–°ç€' ì„¹ì…˜ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”.")
        return []

    articles = []

    # 'æ–°ç€' ì´í›„ í˜•ì œë“¤ì„ í›‘ë‹¤ê°€ ë‹¤ë¥¸ ì„¹ì…˜(ì˜ˆ: 'èª­ã¾ã‚Œã¦ã„ã‚‹è¨˜äº‹') ë‚˜ì˜¤ë©´ ì¤‘ë‹¨
    for sibling in header.find_next_siblings():
        if sibling.name in ["h2", "h3"]:
            # ìƒˆ ì„¹ì…˜ ì‹œì‘ â†’ ì¢…ë£Œ
            break

        for a in sibling.find_all("a", href=True):
            raw_title = a.get_text(strip=True)
            if not raw_title:
                continue
            # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸(ì•„ì´ì½˜ ë“±)ëŠ” ìŠ¤í‚µ
            if len(raw_title) < 8:
                continue

            title_ja, published_at = parse_title_and_datetime(raw_title)

            # ì§€ë‚œ 24ì‹œê°„ ì´ë‚´ í•„í„°
            if published_at is not None:
                if now_jst - published_at > timedelta(hours=24):
                    continue

            url = urljoin(base_url, a["href"])

            articles.append(
                {
                    "title_ja_raw": raw_title,
                    "title_ja": title_ja,
                    "url": url,
                    "published_at": published_at.isoformat() if published_at else None,
                }
            )

    return articles


def translate_title_ja_to_ko(text_ja: str) -> str | None:
    if not text_ja:
        return None
    try:
        result = translator.translate(text_ja, src="ja", dest="ko")
        return result.text
    except Exception as e:
        print(f"êµ¬ê¸€ ë²ˆì—­(ë¬´ë£Œ) ì˜¤ë¥˜: {e}")
        return None


def format_telegram_message(item: dict) -> str:
    title_ja = item.get("title_ja") or item.get("title_ja_raw") or "(ì œëª© ì—†ìŒ)"
    title_ko = item.get("title_ko") or "(ë²ˆì—­ ì‹¤íŒ¨ã… ã… )"
    url = item.get("url", "")
    published_at = item.get("published_at")

    if published_at:
        # ë³´ê¸° ì¢‹ê²Œ í¬ë§·íŒ…
        try:
            dt = datetime.fromisoformat(published_at)
            published_str = dt.astimezone(JST).strftime("%Y-%m-%d %H:%M (%Z)")
        except Exception:
            published_str = published_at
    else:
        published_str = "ì•Œ ìˆ˜ ì—†ìŒ"

    text = (
        "ğŸ“° ì›ë¬¸ ì œëª© (JP): " + title_ja + "\n"
        "ğŸ‡°ğŸ‡· ë²ˆì—­ ì œëª© (KO): " + title_ko + "\n"
        "ğŸ•’ ê²Œì¬ ì‹œê°: " + published_str + "\n"
        "ğŸ”— URL: " + url
    )
    return text


def send_to_telegram(items: list[dict]):
    api_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"

    for item in items:
        text = format_telegram_message(item)
        payload = {
            "chat_id": TELEGRAM_CHAT_ID,
            "text": text,
        }
        try:
            resp = requests.post(api_url, json=payload, timeout=15)
            if not resp.ok:
                print("í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨:", resp.status_code, resp.text)
        except Exception as e:
            print("í…”ë ˆê·¸ë¨ ìš”ì²­ ì—ëŸ¬:", e)


# ======== ì¤‘ë³µ ë°©ì§€ìš© storage ========
def load_seen_urls(path: str) -> set[str]:
    if not os.path.exists(path):
        return set()
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
        # ë¦¬ìŠ¤íŠ¸ í˜¹ì€ dict ëª¨ë‘ ëŒ€ë¹„
        if isinstance(data, list):
            return set(data)
        elif isinstance(data, dict):
            return set(data.get("urls", []))
        else:
            return set()
    except Exception as e:
        print(f"[WARN] storage íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({path}): {e}")
        return set()


def save_seen_urls(path: str, urls: set[str]):
    try:
        with open(path, "w", encoding="utf-8") as f:
            json.dump(sorted(list(urls)), f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[WARN] storage íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ({path}): {e}")


def main():
    ensure_config()

    now_jst = datetime.now(JST)
    print(f"[INFO] í˜„ì¬(JST): {now_jst.isoformat()}")
    print(f"[INFO] Fetching page: {TARGET_URL}")

    html = fetch_html(TARGET_URL)
    articles = extract_new_articles(html, TARGET_URL, now_jst)

    if not articles:
        print("[INFO] ì¡°ê±´ì— ë§ëŠ” ê¸°ì‚¬ê°€ ì—†ì–´ìš” (ì§€ë‚œ 24ì‹œê°„ & æ–°ç€).")
        return

    print(f"[INFO] {len(articles)}ê°œì˜ í›„ë³´ ê¸°ì‚¬ ë°œê²¬")

    # ì¤‘ë³µ ë°©ì§€: ì´ë¯¸ ë³´ë‚¸ URLì€ ì œì™¸
    seen = load_seen_urls(STORAGE_FILE)
    print(f"[INFO] storageì—ì„œ {len(seen)}ê°œ URL ë¡œë“œ")

    new_articles = [item for item in articles if item["url"] not in seen]

    if not new_articles:
        print("[INFO] ìƒˆë¡œ ë³´ë‚¼ ê¸°ì‚¬ê°€ ì—†ì–´ìš” (ëª¨ë‘ ì´ë¯¸ ì „ì†¡ëœ URL).")
        return

    print(f"[INFO] ì‹¤ì œ ì „ì†¡ ëŒ€ìƒ: {len(new_articles)}ê°œ")

    # ì œëª© ë²ˆì—­
    for item in new_articles:
        ja = item["title_ja"]
        print(f"[INFO] Translating: {ja}")
        ko = translate_title_ja_to_ko(ja)
        item["title_ko"] = ko

    # í…”ë ˆê·¸ë¨ ì „ì†¡
    send_to_telegram(new_articles)

    # storage ì—…ë°ì´íŠ¸
    for item in new_articles:
        seen.add(item["url"])
    save_seen_urls(STORAGE_FILE, seen)

    print("[INFO] ì™„ë£Œ!")


if __name__ == "__main__":
    main()
