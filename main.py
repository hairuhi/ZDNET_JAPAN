import os
import re
import json
from datetime import datetime, timedelta
from urllib.parse import urljoin

import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from googletrans import Translator


load_dotenv()

# --- í™˜ê²½ ë³€ìˆ˜ ---

TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

# ê¸°ë³¸ URL (í•„ìš”í•˜ë©´ .envë‚˜ GitHub Secretsì—ì„œ ë®ì–´ì“°ê¸° ê°€ëŠ¥)
JAPAN_SOFTWARE_URL = os.getenv(
    "JAPAN_SOFTWARE_URL", "https://japan.zdnet.com/software/"
)
KOREA_AI_URL = os.getenv(
    "KOREA_AI_URL",
    "https://zdnet.co.kr/newskey/?lstcode=%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5",
)

# ì¤‘ë³µ ë°©ì§€ìš© ìŠ¤í† ë¦¬ì§€ íŒŒì¼ ê²½ë¡œ
STORAGE_PATH = os.getenv("STORAGE_PATH", "sent_articles.json")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (compatible; ZDNetCrawler/1.0; +https://github.com/yourname)"
}

translator = Translator()


class ConfigError(Exception):
    pass


def ensure_config():
    missing = []
    if not TELEGRAM_BOT_TOKEN:
        missing.append("TELEGRAM_BOT_TOKEN")
    if not TELEGRAM_CHAT_ID:
        missing.append("TELEGRAM_CHAT_ID")

    if missing:
        raise ConfigError("í™˜ê²½ë³€ìˆ˜ê°€ ë¶€ì¡±í•´ìš”: " + ", ".join(missing))


# --- ê³µí†µ ìœ í‹¸ ---


def fetch_html(url: str) -> str:
    resp = requests.get(url, headers=HEADERS, timeout=20)
    resp.raise_for_status()
    return resp.text


def load_sent_storage() -> dict:
    if not os.path.exists(STORAGE_PATH):
        return {}
    try:
        with open(STORAGE_PATH, "r", encoding="utf-8") as f:
            data = json.load(f)
            if isinstance(data, dict):
                return data
            return {}
    except Exception:
        return {}


def save_sent_storage(data: dict):
    with open(STORAGE_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def is_within_last_24h(dt: datetime) -> bool:
    """
    ê¸°ì‚¬ ì‹œê°„ì€ JST/KST(+9) ê¸°ì¤€ì´ë¼ê³  ê°€ì •í•˜ê³ ,
    í˜„ì¬ UTCì— +9ì‹œê°„ì„ ë”í•œ 'ë¡œì»¬ ì‹œê°„'ê³¼ ë¹„êµí•´ìš”.
    """
    if dt is None:
        return False
    now_local = datetime.utcnow() + timedelta(hours=9)
    cutoff = now_local - timedelta(hours=24)
    return cutoff <= dt <= now_local


def translate_title_ja_to_ko(text_ja: str) -> str | None:
    if not text_ja:
        return None
    try:
        result = translator.translate(text_ja, src="ja", dest="ko")
        return result.text
    except Exception as e:
        print(f"êµ¬ê¸€ ë²ˆì—­(ë¬´ë£Œ) ì˜¤ë¥˜: {e}")
        return None


def format_telegram_message(item: dict) -> str:
    source = item.get("source", "")
    url = item.get("url", "")
    published_at = item.get("published_at")

    if source == "zdnet_jp":
        title_ja = item.get("title_ja") or "(ì œëª© ì—†ìŒ)"
        title_ko = item.get("title_ko") or "(ë²ˆì—­ ì‹¤íŒ¨ã… ã… )"
        source_label = "ğŸ‡¯ğŸ‡µ ZDNet Japan (Software)"
        text = (
            f"{source_label}\n"
            f"ğŸ“° ì›ë¬¸ ì œëª©(JP): {title_ja}\n"
            f"ğŸ‡°ğŸ‡· ë²ˆì—­ ì œëª©(KO): {title_ko}\n"
        )
    elif source == "zdnet_kr_ai":
        title_ko = item.get("title_ko") or "(ì œëª© ì—†ìŒ)"
        source_label = "ğŸ‡°ğŸ‡· ZDNet Korea (AI)"
        text = f"{source_label}\nğŸ“° ì œëª©: {title_ko}\n"
    else:
        title = item.get("title") or "(ì œëª© ì—†ìŒ)"
        source_label = "ğŸ“° ZDNet"
        text = f"{source_label}\nì œëª©: {title}\n"

    if isinstance(published_at, datetime):
        text += f"ğŸ•’ ê¸°ì‚¬ ì‹œê°: {published_at.strftime('%Y-%m-%d %H:%M')}\n"

    text += f"ğŸ”— URL: {url}"
    return text


def send_to_telegram(items: list[dict]):
    api_url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"

    for item in items:
        text = format_telegram_message(item)
        payload = {
            "chat_id": TELEGRAM_CHAT_ID,
            "text": text,
        }
        try:
            resp = requests.post(api_url, json=payload, timeout=20)
            if not resp.ok:
                print("í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨:", resp.status_code, resp.text)
        except Exception as e:
            print("í…”ë ˆê·¸ë¨ ìš”ì²­ ì—ëŸ¬:", e)


# --- ì¼ë³¸ ZDNet (software) ---


def clean_title_jp(raw_title: str) -> str:
    """
    ì œëª© ë’¤ì— ë¶™ì€ ë‚ ì§œ/ì‹œê°„(ì˜ˆ: ' ... 2025-11-16 08:00') ë¶€ë¶„ ì œê±°.
    """
    if not raw_title:
        return ""
    cleaned = re.sub(r"\s+\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}.*$", "", raw_title).strip()
    return cleaned


def extract_new_articles_jp_list(html: str, base_url: str) -> list[dict]:
    """
    ì¼ë³¸ ZDNet software í˜ì´ì§€ì—ì„œ 'æ–°ç€' ì„¹ì…˜ ìœ„ì£¼ë¡œ ê¸°ì‚¬ ëª©ë¡ì„ ê°€ì ¸ì™€ìš”.
    ì—¬ê¸°ì„œëŠ” 'ì œëª© + URL'ê¹Œì§€ë§Œ ë½‘ê³ , ì‹œê°„ ì •ë³´ëŠ” ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ ë‹¤ì‹œ ê°€ì ¸ì™€ìš”.
    """
    soup = BeautifulSoup(html, "html.parser")

    header = soup.find(
        lambda tag: tag.name in ["h2", "h3"]
        and tag.get_text(strip=True).startswith("æ–°ç€")
    )
    if not header:
        print("[JP] æ–°ç€ ì„¹ì…˜ì„ ëª» ì°¾ì•˜ì–´ìš” ã… ã… ")
        return []

    articles: list[dict] = []

    # 'æ–°ç€' ì´í›„ í˜•ì œë“¤ì„ ëŒë‹¤ê°€ ë‹¤ë¥¸ í° ì„¹ì…˜(h2/h3)ì´ ë‚˜ì˜¤ë©´ ì¢…ë£Œ
    for sibling in header.find_next_siblings():
        if sibling.name in ["h2", "h3"]:
            break

        for a in sibling.find_all("a", href=True):
            title = a.get_text(strip=True)
            if not title:
                continue
            if len(title) < 8:
                continue

            url = urljoin(base_url, a["href"])
            articles.append(
                {
                    "source": "zdnet_jp",
                    "title_ja_raw": title,
                    "title_ja": clean_title_jp(title),
                    "url": url,
                }
            )

    return articles


def fetch_published_at_jp(article_url: str) -> datetime | None:
    """
    ì¼ë³¸ ZDNet ê¸°ì‚¬ ë³¸ë¬¸ì—ì„œ '2025-11-16 08:00' ê°™ì€ í˜•ì‹ìœ¼ë¡œ ëœ ë‚ ì§œë¥¼ ì°¾ê³  datetimeìœ¼ë¡œ ë³€í™˜.
    """
    try:
        html = fetch_html(article_url)
    except Exception as e:
        print(f"[JP] ê¸°ì‚¬ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {article_url} ({e})")
        return None

    soup = BeautifulSoup(html, "html.parser")
    # '2025-11-16 08:00' ê°™ì€ ë¬¸ìì—´ì„ í¬í•¨í•œ í…ìŠ¤íŠ¸ ë…¸ë“œ ì°¾ê¸°
    text_node = soup.find(string=re.compile(r"\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}"))
    if not text_node:
        return None

    m = re.search(r"(\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2})", text_node)
    if not m:
        return None

    dt_str = m.group(1)
    try:
        dt = datetime.strptime(dt_str, "%Y-%m-%d %H:%M")
        return dt
    except ValueError:
        return None


def collect_recent_articles_jp() -> list[dict]:
    print(f"[JP] Fetching list page: {JAPAN_SOFTWARE_URL}")
    html = fetch_html(JAPAN_SOFTWARE_URL)
    candidates = extract_new_articles_jp_list(html, JAPAN_SOFTWARE_URL)
    print(f"[JP] í›„ë³´ ê¸°ì‚¬ {len(candidates)}ê°œ ë°œê²¬")

    recent: list[dict] = []
    for item in candidates:
        url = item["url"]
        dt = fetch_published_at_jp(url)
        if not dt:
            print(f"[JP] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, ìŠ¤í‚µ: {url}")
            continue
        item["published_at"] = dt
        if is_within_last_24h(dt):
            recent.append(item)

    print(f"[JP] ì§€ë‚œ 24ì‹œê°„ ê¸°ì‚¬ {len(recent)}ê°œ")
    return recent


# --- í•œêµ­ ZDNet (ì¸ê³µì§€ëŠ¥ ë¦¬ìŠ¤íŠ¸) ---


def extract_new_articles_kr_ai_list(html: str, base_url: str) -> list[dict]:
    """
    ì¸ê³µì§€ëŠ¥ ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì œëª© + URLë§Œ ì¶”ì¶œ.
    """
    soup = BeautifulSoup(html, "html.parser")

    header = soup.find(
        lambda tag: tag.name in ["h2", "h3"]
        and "ì¸ê³µì§€ëŠ¥ ìµœì‹ ë‰´ìŠ¤" in tag.get_text()
    )
    if not header:
        print("[KR] 'ì¸ê³µì§€ëŠ¥ ìµœì‹ ë‰´ìŠ¤' ì„¹ì…˜ì„ ëª» ì°¾ì•˜ì–´ìš” ã… ã… ")
        return []

    articles: list[dict] = []

    # 'ì¸ê³µì§€ëŠ¥ ìµœì‹ ë‰´ìŠ¤' ì´í›„ í˜•ì œë“¤ì„ ëŒë‹¤ê°€ 'ì§€ê¸ˆ ëœ¨ëŠ” ê¸°ì‚¬' ì„¹ì…˜(h2/h3) ë‚˜ì˜¤ë©´ ì¢…ë£Œ
    for sibling in header.find_next_siblings():
        if sibling.name in ["h2", "h3"] and "ì§€ê¸ˆ ëœ¨ëŠ” ê¸°ì‚¬" in sibling.get_text():
            break

        for a in sibling.find_all("a", href=True):
            href = a["href"]
            if "/view/?no=" not in href:
                continue
            title = a.get_text(strip=True)
            if not title:
                continue
            url = urljoin(base_url, href)
            articles.append(
                {
                    "source": "zdnet_kr_ai",
                    "title_ko": title,
                    "url": url,
                }
            )

    return articles


def fetch_published_at_kr(article_url: str) -> datetime | None:
    """
    í•œêµ­ ZDNet ê¸°ì‚¬ í˜ì´ì§€ì—ì„œ
    'ì…ë ¥ :2025/11/14 17:46    ìˆ˜ì •: 2025/11/14 17:48'
    ê°™ì€ ë¶€ë¶„ì—ì„œ 'ì…ë ¥' ì‹œê°ì„ íŒŒì‹±.
    """
    try:
        html = fetch_html(article_url)
    except Exception as e:
        print(f"[KR] ê¸°ì‚¬ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {article_url} ({e})")
        return None

    soup = BeautifulSoup(html, "html.parser")
    text_node = soup.find(string=re.compile(r"ì…ë ¥\s*:?\s*\d{4}/\d{2}/\d{2}\s+\d{2}:\d{2}"))
    if not text_node:
        return None

    m = re.search(r"ì…ë ¥\s*:?\s*(\d{4}/\d{2}/\d{2}\s+\d{2}:\d{2})", text_node)
    if not m:
        return None

    dt_str = m.group(1)
    try:
        dt = datetime.strptime(dt_str, "%Y/%m/%d %H:%M")
        return dt
    except ValueError:
        return None


def collect_recent_articles_kr_ai() -> list[dict]:
    print(f"[KR] Fetching AI list page: {KOREA_AI_URL}")
    html = fetch_html(KOREA_AI_URL)
    candidates = extract_new_articles_kr_ai_list(html, KOREA_AI_URL)
    print(f"[KR] í›„ë³´ ê¸°ì‚¬ {len(candidates)}ê°œ ë°œê²¬")

    recent: list[dict] = []
    for item in candidates:
        url = item["url"]
        dt = fetch_published_at_kr(url)
        if not dt:
            print(f"[KR] ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, ìŠ¤í‚µ: {url}")
            continue
        item["published_at"] = dt
        if is_within_last_24h(dt):
            recent.append(item)

    print(f"[KR] ì§€ë‚œ 24ì‹œê°„ ê¸°ì‚¬ {len(recent)}ê°œ")
    return recent


# --- ë©”ì¸ ë¡œì§ ---


def main():
    ensure_config()

    sent_storage = load_sent_storage()
    if not isinstance(sent_storage, dict):
        sent_storage = {}

    # 1) ê° ì‚¬ì´íŠ¸ì—ì„œ ì§€ë‚œ 24ì‹œê°„ ê¸°ì‚¬ ìˆ˜ì§‘
    jp_articles = collect_recent_articles_jp()
    kr_articles = collect_recent_articles_kr_ai()

    all_candidates: list[dict] = jp_articles + kr_articles
    print(f"[ALL] ì´ í›„ë³´ ê¸°ì‚¬ {len(all_candidates)}ê°œ")

    # 2) ì¤‘ë³µ(ì´ë¯¸ ë³´ë‚¸ URL) ì œê±° + ì¼ë³¸ ê¸°ì‚¬ ì œëª© ë²ˆì—­
    new_items: list[dict] = []
    for item in all_candidates:
        url = item["url"]
        if url in sent_storage:
            print(f"[SKIP] ì´ë¯¸ ì „ì†¡í•œ ê¸°ì‚¬ë¼ ìŠ¤í‚µ: {url}")
            continue

        if item.get("source") == "zdnet_jp":
            ja = item.get("title_ja")
            ko = translate_title_ja_to_ko(ja)
            item["title_ko"] = ko

        # ìƒˆ ê¸°ì‚¬ë¡œ ì¸ì • â†’ ìŠ¤í† ë¦¬ì§€ì— ê¸°ë¡
        sent_storage[url] = datetime.utcnow().isoformat()
        new_items.append(item)

    print(f"[ALL] ìƒˆë¡œ ë³´ë‚¼ ê¸°ì‚¬ {len(new_items)}ê°œ")

    if not new_items:
        print("[INFO] ë³´ë‚¼ ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ì–´ìš”.")
        return

    # 3) í…”ë ˆê·¸ë¨ ì „ì†¡
    send_to_telegram(new_items)

    # 4) ìŠ¤í† ë¦¬ì§€ ì €ì¥
    save_sent_storage(sent_storage)
    print("[INFO] ì™„ë£Œ!")


if __name__ == "__main__":
    main()
